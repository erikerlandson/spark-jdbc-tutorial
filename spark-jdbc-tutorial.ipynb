{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JDBC I/O in Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial you'll learn the basics of reading and writing Apache Spark DataFrames to an SQL database,\n",
    "using Apache Spark's JDBC API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tutorial Requirements\n",
    "This tutorial assumes you have write privileges (including table-create privileges) to a postgresql database.\n",
    "Instructions below will show you how to fill in your database connect information.\n",
    "\n",
    "This tutorial includes the Maven coordinates for using the postgresql JDBC driver.\n",
    "If you wish to connect to a different SQL database vendor, and you have access to the proper JDBC driver Maven package (or jar file), you should be able to run this demo against a non-postgresql DB.\n",
    "\n",
    "To run this tutorial you also need to connect to an Apache Spark cluster.\n",
    "You can enter the spark master hostname in the cell that creates the Spark Session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apache Spark Configurations\n",
    "\n",
    "Spark can receive its configuration parameters from a \n",
    "[variety](https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties)\n",
    "of channels.\n",
    "In general, configurations set via a `SparkConf` object (as below) will override all other\n",
    "configurations.\n",
    "However, there are a few glitches in this rule, and the `spark.package.jars` parameter is one of them,\n",
    "which is important for this tutorial.\n",
    "To maximize clarity, this notebook unsets `PYSPARK_SUBMIT_ARGS` in favor of doing all\n",
    "configurations using a `SparkConf` object so that it is easy to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable this so that configuration of 'spark.jars.packages' works correctly\n",
    "if 'PYSPARK_SUBMIT_ARGS' in os.environ:\n",
    "    del os.environ['PYSPARK_SUBMIT_ARGS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JDBC Drivers\n",
    "In order to work with Spark's JDBC API, you'll need to provide Spark with a JDBC driver.\n",
    "Many drivers are available as Maven-style packages,\n",
    "such as the driver for\n",
    "[postgresql](https://www.postgresql.org/)\n",
    "in the cell below.\n",
    "\n",
    "Consuming JDBC drivers via Maven coordinates and `spark.jars.packages` is convenient,\n",
    "since Spark will automatically download such packages and install them on Spark executors.\n",
    "\n",
    "In some cases, database vendors may provide a \"raw\" jar file instead of a Maven package.\n",
    "The code comments below show a Spark connection alternative that specifies the driver as a jar file\n",
    "using the `spark.jars` configuration parameter.\n",
    "Installing and configuring jar files directly may also be well suited for building container images\n",
    "that can run on platforms like Kubernetes or OpenShift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f66b88087f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "\n",
    "# Instantiate a spark configuration object to receive settings\n",
    "spark_conf = SparkConf()\n",
    "\n",
    "# Maven coordinates for package containing JDBC drivers\n",
    "jdbc_driver_packages = 'org.postgresql:postgresql:42.2.9'\n",
    "\n",
    "# Configure spark to see the postgresql driver package\n",
    "spark_conf.set('spark.jars.packages', jdbc_driver_packages)\n",
    "\n",
    "# Alternative method: directly list path to your JDBC driver jar (or jars)\n",
    "# jdbc_driver_jars = '/path/to/postgresql-42.2.9.jar'\n",
    "# spark_conf.set('spark.jars', jdbc_driver_jars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining a Spark Session\n",
    "\n",
    "Before we can begin, we need to attach to a running Apache Spark cluster.\n",
    "In this cell, you'll set the hostname of the Spark master to connect to.\n",
    "The `SparkConf` settings instruct my session to use just a single executor with 1 cpu core. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# The name of your Spark cluster hostname or ip address\n",
    "spark_cluster = 'spark-cluster-eje'\n",
    "\n",
    "# Configure some basic spark cluster sizing parameters\n",
    "spark_conf.set('spark.cores.max', 1)\n",
    "spark_conf.set('spark.executor.cores', '1')\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('spark://{cluster}:7077'.format(cluster=spark_cluster)) \\\n",
    "    .appName('Spark-JDBC-Demo') \\\n",
    "    .config(conf = spark_conf) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking JDBC Driver Configuration\n",
    "\n",
    "As we discussed above, Spark has some subtle bugs in its normal configuration precedence orderings when it comes to `spark.package.jars` and `spark.jars`.\n",
    "You can use the `getConf()` method to sanity-check the final settings that Spark is using.\n",
    "In this cell, we are checking that the jar-files for our postgresql JDBC driver will actually be visible in Spark's classpath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'postgresql' in spark.sparkContext.getConf().get('spark.jars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Data\n",
    "\n",
    "For the purposes of this tutorial, we'll be working with a small example data table.\n",
    "The first column is some consecutive integers, and the second is the squares of the first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|xsq|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  4|\n",
      "|  3|  9|\n",
      "|  4| 16|\n",
      "+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_raw = [(x, x*x) for x in range(1000)]\n",
    "data_df = spark.createDataFrame(data_raw, ['x', 'xsq'])\n",
    "data_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark JDBC URL and Properties\n",
    "\n",
    "Apache Spark JDBC calls take two data structures to specify a database connection.\n",
    "The first is a string containing a JDBC connection URL.\n",
    "Such a URL typically includes the following db connect info:\n",
    "* vendor (here, 'postgresql')\n",
    "* hostname\n",
    "* port\n",
    "* database name\n",
    "\n",
    "The second structure is a property map.\n",
    "In python this is a `dict` structure, containing:\n",
    "* db user name\n",
    "* password\n",
    "* Java class name of the JDBC driver\n",
    "\n",
    "For some vendors, other properties are expected.\n",
    "A common additional property is `sslConnection`, as shown below in the comments.\n",
    "\n",
    "Remember, it is best practice to store username and password in environment variables or other forms that can be set without explicitly typing security information in your code!\n",
    "\n",
    "The exact syntax of the JDBC URL varies from vendor to vendor.\n",
    "Refer to the vendor's JDBC driver documentation for connection specifics.\n",
    "Vendors that publish JDBC drivers usable by Spark will generally include Spark example connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_jdbc_url = 'jdbc:postgresql://{host}:{port}/{database}'.format( \\\n",
    "    host     = 'postgresql', \\\n",
    "    port     = '5432', \\\n",
    "    database = 'demodb')\n",
    "\n",
    "spark_jdbc_prop = { \\\n",
    "    'user':     'eje', \\\n",
    "    'password': 'eje12345', \\\n",
    "    'driver':   'org.postgresql.Driver'\n",
    "    # 'sslConnection': 'false'\n",
    "    # Some DB vendors expect other connection properties.\n",
    "    # Setting 'sslConnection' is one common vendor-specific parameter\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing a DataFrame with JDBC\n",
    "\n",
    "The following Spark call uses our database connect info above to write our example data to a database table.\n",
    "The two write modes are `overwrite` and `append`.\n",
    "Note that in `overwrite` mode you must have both write and table create privileges on your db!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.write.jdbc( \\\n",
    "    table      = 'demo', \\\n",
    "    mode       = 'overwrite', \\\n",
    "    url        = spark_jdbc_url, \\\n",
    "    properties = spark_jdbc_prop \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading a DataFrame with JDBC\n",
    "\n",
    "The basics of reading a DataFrame from a JDBC query are (almost) as simple as writing.\n",
    "The database connection information is the same.\n",
    "Here, we must specify a database query, written in the vendor's supported dialect of SQL.\n",
    "A query can be very simple, as in the example below, or hundreds of lines of complex SQL code!\n",
    "It is generally best practice to set the query string separately, as in this example.\n",
    "\n",
    "Note that in the `read.jdbc` call below, we have enclosed the raw query in parentheses and assigned it a temporary view `tmp`, which Spark requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|xsq|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  4|\n",
      "|  3|  9|\n",
      "|  4| 16|\n",
      "+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'select * from demo'\n",
    "\n",
    "query_df = spark.read.jdbc( \\\n",
    "    table      = '({q}) tmp'.format(q=query), \\\n",
    "    url        = spark_jdbc_url, \\\n",
    "    properties = spark_jdbc_prop \\\n",
    ")\n",
    "\n",
    "query_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Perils of Partitioning\n",
    "\n",
    "Apache Spark's scalable compute model depends on being able to break data into multiple partitions so that it can parallelize work across each partition.\n",
    "Let's look at how our dataframe got partitioned when we read it above.\n",
    "The following cell prints out the number of partitions and the number of records in each partition.\n",
    "\n",
    "As you can see from the output, Spark put all the records in our query into a single partition!\n",
    "For our small example data, this is not a problem.\n",
    "However, if we are working with large volumes of data, this is bad news!\n",
    "With all our data in a single partition, Spark cannot process our data in parallel.\n",
    "Worse, if a sufficiently large query result is pushed into a single executor,\n",
    "it can easily cause an out of memory error and crash the executor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partitions: 1\n",
      "sizes: [1000]\n"
     ]
    }
   ],
   "source": [
    "print(\"partitions: {np}\\nsizes: {sz}\".format( \\\n",
    "    np = query_df.rdd.getNumPartitions(), \\\n",
    "    sz = query_df.rdd.mapPartitions(lambda itr: [len(list(itr))]).collect() \\\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proper DataFrame Partitioning with JDBC\n",
    "\n",
    "Fortunately, Spark provides a way to perform JDBC reads and correctly partition the result.\n",
    "\n",
    "When you read data into a Spark DataFrame using a JDBC query,\n",
    "Spark needs extra information about which data from the DB to put into each partition.\n",
    "Spark does this by generating one query for each partition, under the hood.\n",
    "To enable this, you must provide your JDBC read with a list of \"partitioning predicates\":\n",
    "Spark will generate its DataFrame with one partition for each predicate you give it, and the data that goes into each partition is the data that is `true` for the corresponding predicate.\n",
    "\n",
    "For example, if we wanted to partition our data into 3 queries,\n",
    "we might want spark to use the following 3 queries:\n",
    "\n",
    "```sql\n",
    "select x, xsq from demo where mod(x, 3) = 0  /* query for 1st partition */\n",
    "select x, xsq from demo where mod(x, 3) = 1  /* query for 2nd partition */\n",
    "select x, xsq from demo where mod(x, 3) = 2  /* query for 3rd partition */\n",
    "```\n",
    "\n",
    "Spark expects us to provide a list that looks like this:\n",
    "```python\n",
    "[ 'mod(x, 3) = 0', 'mod(x, 3) = 1', 'mod(x, 3) = 2' ]\n",
    "```\n",
    "\n",
    "Notice that these clauses have been designed so that every record ends up in exactly one of our partitions,\n",
    "and also that our partition sizes should be roughly equal, with 1 out of 3 records satisfying each.\n",
    "\n",
    "In practice, we may very well want to create a large number of these partitioning predicates,\n",
    "and so it is a good idea to generate them with a function,\n",
    "such as the `qpreds` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mod(x, 3) = 0', 'mod(x, 3) = 1', 'mod(x, 3) = 2']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def qpreds(n, rowcol):\n",
    "    return [\"mod({rc}, {np}) = {mk}\".format(mk=k, np=n, rc=rowcol) for k in range(n)]\n",
    "\n",
    "qpreds(3, 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A JDBC Read With Partitioning\n",
    "\n",
    "With our `qpreds` function above, we can easily add the additional `predicates` parameter\n",
    "to our JDBC read so that Spark can create a well partitioned DataFrame from our query.\n",
    "In our example below, we configure our predicates for 5 partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|xsq|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  5| 25|\n",
      "| 10|100|\n",
      "| 15|225|\n",
      "| 20|400|\n",
      "+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a JDBC read with proper partitioning\n",
    "query = 'select * from demo'\n",
    "\n",
    "query_df_pp = spark.read.jdbc( \\\n",
    "    table      = '({q}) tmp'.format(q=query), \\\n",
    "    url        = spark_jdbc_url, \\\n",
    "    properties = spark_jdbc_prop, \\\n",
    "    predicates = qpreds(5, 'x') \\\n",
    ")\n",
    "query_df_pp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying Partitions\n",
    "Now, when we check our paritions, we see a well partitioned DataFrame that has the 5 partitions we desired,\n",
    "with the query results evenly distributed among the partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partitions: 5\n",
      "sizes: [200, 200, 200, 200, 200]\n"
     ]
    }
   ],
   "source": [
    "print(\"partitions: {np}\\nsizes: {sz}\".format( \\\n",
    "    np = query_df_pp.rdd.getNumPartitions(), \\\n",
    "    sz = query_df_pp.rdd.mapPartitions(lambda itr: [len(list(itr))]).collect() \\\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Partitioning Techniques\n",
    "\n",
    "In our previous examples we took advantage of having a column `x` in our data that was both an integer and had a distribution of values (uniform) that was easy to generate equal-sized partitions from.\n",
    "In real data we may not have this kind of convenient data to partition with,\n",
    "but there are a couple techniques that we can use with any data.\n",
    "\n",
    "The first technique is hashing.\n",
    "For SQL dialects that support a hashing function, you can pick a column (or columns) to apply a hash to,\n",
    "and then take the modulus of the resulting hash value.\n",
    "A hypothetical example of such predicates might look like this:\n",
    "\n",
    "```sql\n",
    "mod(vendor_hash(my_column, 3)) = 0\n",
    "mod(vendor_hash(my_column, 3)) = 1\n",
    "mod(vendor_hash(my_column, 3)) = 2\n",
    "```\n",
    "\n",
    "If you use this technique, you may need to tweak your `qpreds` function to generate predicates of this form.\n",
    "\n",
    "Not all SQL dialects have this kind of hash function, but there is almost always some variation on assigning\n",
    "a unique integer to each query output row.\n",
    "In postgresql, this function is `row_number()`, and we can add it to our query `select`.\n",
    "In the following example, we have added a row number clause to our query.\n",
    "In postgresql, this must include an `over` clause to tell it what ordering you wish the numbering to use.\n",
    "When you refer to this new column, you just use `row_number` as you can see in the `qpred` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+\n",
      "|  x|xsq|row_number|\n",
      "+---+---+----------+\n",
      "|  4| 16|         5|\n",
      "|  9| 81|        10|\n",
      "| 14|196|        15|\n",
      "| 19|361|        20|\n",
      "| 24|576|        25|\n",
      "+---+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The additional row_number clause is not necessary if you are partitioning via \n",
    "# an existing integer field, or hashing, etc.\n",
    "query = 'select *, row_number() over (order by x) from demo'\n",
    "\n",
    "query_df_pp2 = spark.read.jdbc( \\\n",
    "    table      = '({q}) tmp'.format(q=query), \\\n",
    "    url        = spark_jdbc_url, \\\n",
    "    properties = spark_jdbc_prop, \\\n",
    "    predicates = qpreds(5, 'row_number') \\\n",
    ")\n",
    "query_df_pp2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Row Numbering Adds a Column\n",
    "You can see from the output above that using the row-numbering technique causes adds that column to your\n",
    "query results.\n",
    "You may want to drop this column if you are generating output to some other channel.\n",
    "\n",
    "Lastly, we check our partitioning to see that it worked correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partitions: 5\n",
      "sizes: [200, 200, 200, 200, 200]\n"
     ]
    }
   ],
   "source": [
    "print(\"partitions: {np}\\nsizes: {sz}\".format( \\\n",
    "    np = query_df_pp2.rdd.getNumPartitions(), \\\n",
    "    sz = query_df_pp2.rdd.mapPartitions(lambda itr: [len(list(itr))]).collect() \\\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
